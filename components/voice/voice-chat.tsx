'use client'

import { useState, useEffect, useRef } from 'react'
import { Button } from '@/components/ui/button'
import { Mic, MicOff, Volume2, VolumeX, Phone, PhoneOff } from 'lucide-react'
import { toast } from 'sonner'
import { VoiceSettingsDialog, type VoiceSettings, DEFAULT_SETTINGS } from './voice-settings'

// æµè§ˆå™¨ API ç±»å‹å®šä¹‰
interface SpeechRecognition extends EventTarget {
  continuous: boolean
  interimResults: boolean
  lang: string
  start(): void
  stop(): void
  abort(): void
  onresult: ((event: SpeechRecognitionEvent) => void) | null
  onend: (() => void) | null
  onerror: ((event: SpeechRecognitionErrorEvent) => void) | null
}

interface SpeechRecognitionEvent {
  resultIndex: number
  results: SpeechRecognitionResultList
}

interface SpeechRecognitionResultList {
  length: number
  item(index: number): SpeechRecognitionResult
  [index: number]: SpeechRecognitionResult
}

interface SpeechRecognitionResult {
  length: number
  item(index: number): SpeechRecognitionAlternative
  [index: number]: SpeechRecognitionAlternative
  isFinal: boolean
}

interface SpeechRecognitionAlternative {
  transcript: string
  confidence: number
}

interface SpeechRecognitionErrorEvent {
  error: string
  message: string
}

declare global {
  interface Window {
    SpeechRecognition: {
      new (): SpeechRecognition
    }
    webkitSpeechRecognition: {
      new (): SpeechRecognition
    }
  }
}

interface VoiceChatProps {
  onTranscript?: (text: string) => void
  onSpeak?: (text: string) => void
  autoSpeak?: boolean // æ˜¯å¦è‡ªåŠ¨æœ—è¯» AI å›å¤
  continuousMode?: boolean // è¿ç»­å¯¹è¯æ¨¡å¼ï¼ˆåƒæ‰“ç”µè¯ä¸€æ ·ï¼‰
  onContinuousModeChange?: (enabled: boolean) => void
}

/**
 * è¯­éŸ³èŠå¤©ç»„ä»¶
 * æ”¯æŒè¯­éŸ³è¾“å…¥å’Œè¾“å‡ºï¼Œå‚è€ƒè±†åŒ…çš„äº¤äº’æ–¹å¼
 * - æŒ‰ä½æŒ‰é’®è¯´è¯ï¼ˆå…¼å®¹ç§»åŠ¨ç«¯ï¼‰
 * - éŸ³è‰²ã€è¯­é€Ÿã€éŸ³è°ƒæ§åˆ¶
 * - è¿ç»­å¯¹è¯æ¨¡å¼
 */
export function VoiceChat({
  onTranscript,
  onSpeak,
  autoSpeak = true,
  continuousMode = false,
  onContinuousModeChange,
}: VoiceChatProps) {
  const [isListening, setIsListening] = useState(false)
  const [isSpeaking, setIsSpeaking] = useState(false)
  const [isSupported, setIsSupported] = useState(false)
  const [voiceSettings, setVoiceSettings] = useState<VoiceSettings>(DEFAULT_SETTINGS)
  const [availableVoices, setAvailableVoices] = useState<SpeechSynthesisVoice[]>([])
  const recognitionRef = useRef<SpeechRecognition | null>(null)
  const synthesisRef = useRef<SpeechSynthesis | null>(null)
  const currentUtteranceRef = useRef<SpeechSynthesisUtterance | null>(null)
  const isPressingRef = useRef(false) // ç”¨äºè·Ÿè¸ªæŒ‰é’®æŒ‰ä¸‹çŠ¶æ€

  // æ£€æŸ¥æµè§ˆå™¨æ”¯æŒ
  useEffect(() => {
    const checkSupport = () => {
      const hasRecognition = 'webkitSpeechRecognition' in window || 'SpeechRecognition' in window
      const hasSynthesis = 'speechSynthesis' in window
      setIsSupported(hasRecognition && hasSynthesis)
      
      if (!hasRecognition) {
        console.warn('Speech Recognition not supported')
      }
      if (!hasSynthesis) {
        console.warn('Speech Synthesis not supported')
      }
    }
    checkSupport()
  }, [])

  // åŠ è½½å¯ç”¨è¯­éŸ³åˆ—è¡¨
  useEffect(() => {
    if (!('speechSynthesis' in window)) return

    const loadVoices = () => {
      const voices = window.speechSynthesis.getVoices()
      setAvailableVoices(voices)
    }

    loadVoices()
    // æŸäº›æµè§ˆå™¨éœ€è¦ç­‰å¾… voiceschanged äº‹ä»¶
    if ('onvoiceschanged' in window.speechSynthesis) {
      window.speechSynthesis.onvoiceschanged = loadVoices
    }
  }, [])

  // åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«
  useEffect(() => {
    if (!isSupported) return

    const SpeechRecognition = (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition
    if (!SpeechRecognition) return

    const recognition = new SpeechRecognition()
    recognition.continuous = true // å§‹ç»ˆä½¿ç”¨è¿ç»­æ¨¡å¼ï¼Œä¿æŒè¯†åˆ«çŠ¶æ€
    recognition.interimResults = false // åªè¿”å›æœ€ç»ˆç»“æœ
    recognition.lang = 'zh-CN' // ä¸­æ–‡

    recognition.onstart = () => {
      setIsListening(true)
      // ä¸æ˜¾ç¤º toastï¼Œå› ä¸ºå·²ç»åœ¨ toggleListening ä¸­æ˜¾ç¤ºäº†
    }

    recognition.onresult = (event: SpeechRecognitionEvent) => {
      // è·å–æ‰€æœ‰è¯†åˆ«ç»“æœå¹¶åˆå¹¶ï¼ˆè¿½åŠ æ¨¡å¼ï¼‰
      let fullTranscript = ''
      for (let i = 0; i < event.results.length; i++) {
        fullTranscript += event.results[i][0].transcript
      }
      
      console.log('ğŸ¤ è¯†åˆ«ç»“æœ:', fullTranscript)
      
      if (fullTranscript.trim()) {
        // ä¼ é€’å®Œæ•´çš„è¯†åˆ«æ–‡æœ¬ï¼ˆåŒ…å«ä¹‹å‰çš„å†…å®¹ï¼‰
        onTranscript?.(fullTranscript)
        // ä¸è‡ªåŠ¨åœæ­¢ï¼Œä¿æŒè¯†åˆ«çŠ¶æ€ç›´åˆ°ç”¨æˆ·ç‚¹å‡»åœæ­¢
      }
    }

    recognition.onerror = (event: any) => {
      console.error('è¯­éŸ³è¯†åˆ«é”™è¯¯:', event.error)
      
      if (event.error === 'no-speech') {
        // åœ¨è¿ç»­æ¨¡å¼ä¸‹ï¼Œno-speech æ˜¯æ­£å¸¸çš„ï¼Œä¸æ˜¾ç¤ºé”™è¯¯
        if (!continuousMode) {
          toast.error('æœªæ£€æµ‹åˆ°è¯­éŸ³ï¼Œè¯·é‡è¯•')
        }
      } else if (event.error === 'not-allowed') {
        toast.error('è¯·å…è®¸éº¦å…‹é£æƒé™')
        setIsListening(false)
      } else if (event.error !== 'aborted') {
        // aborted æ˜¯ç”¨æˆ·ä¸»åŠ¨åœæ­¢ï¼Œä¸æ˜¾ç¤ºé”™è¯¯
        toast.error('è¯­éŸ³è¯†åˆ«å¤±è´¥: ' + event.error)
        setIsListening(false)
      }
    }

    recognition.onend = () => {
      // å¦‚æœç”¨æˆ·è¿˜åœ¨ä¿æŒè¯†åˆ«çŠ¶æ€ï¼ˆç‚¹å‡»äº†å¼€å§‹ä½†è¿˜æ²¡ç‚¹å‡»åœæ­¢ï¼‰ï¼Œè‡ªåŠ¨é‡å¯
      if (isPressingRef.current) {
        // å»¶è¿Ÿé‡å¯ï¼Œé¿å…ç«‹å³é‡å¯å¯¼è‡´çš„é—®é¢˜
        setTimeout(() => {
          if (isPressingRef.current && recognitionRef.current) {
            try {
              recognitionRef.current.start()
            } catch (error) {
              // å¯èƒ½å·²ç»åœ¨è¿è¡Œï¼Œå¿½ç•¥é”™è¯¯
              console.log('è¯­éŸ³è¯†åˆ«é‡å¯:', error)
            }
          }
        }, 100)
      } else {
        // ç”¨æˆ·ä¸»åŠ¨åœæ­¢ï¼Œæ›´æ–°çŠ¶æ€
        setIsListening(false)
      }
    }

    recognitionRef.current = recognition

    return () => {
      if (recognitionRef.current) {
        recognitionRef.current.stop()
      }
    }
  }, [isSupported, onTranscript, continuousMode])

  // åˆå§‹åŒ–è¯­éŸ³åˆæˆ
  useEffect(() => {
    if (!isSupported) return

    synthesisRef.current = window.speechSynthesis

    return () => {
      // æ¸…ç†æœªå®Œæˆçš„è¯­éŸ³
      if (currentUtteranceRef.current) {
        synthesisRef.current?.cancel()
      }
    }
  }, [isSupported])

  // åˆ‡æ¢è¯­éŸ³è¾“å…¥çŠ¶æ€ï¼ˆç‚¹å‡»åˆ‡æ¢æ¨¡å¼ï¼‰
  const toggleListening = () => {
    if (!recognitionRef.current) {
      toast.error('è¯­éŸ³è¯†åˆ«æœªåˆå§‹åŒ–')
      return
    }

    if (isListening) {
      // åœæ­¢è¯†åˆ«
      isPressingRef.current = false
      try {
        recognitionRef.current.stop()
        // ç­‰å¾… onend äº‹ä»¶æ›´æ–°çŠ¶æ€
        setTimeout(() => {
          if (!isPressingRef.current) {
            setIsListening(false)
            toast.info('ğŸ¤ å·²åœæ­¢è¯­éŸ³è¯†åˆ«')
          }
        }, 200)
      } catch (error) {
        console.error('åœæ­¢è¯­éŸ³è¯†åˆ«å¤±è´¥:', error)
        setIsListening(false)
        isPressingRef.current = false
        toast.info('ğŸ¤ å·²åœæ­¢è¯­éŸ³è¯†åˆ«')
      }
    } else {
      // å¼€å§‹è¯†åˆ«
      isPressingRef.current = true
      try {
        recognitionRef.current.start()
        toast.info('ğŸ¤ å·²å¼€å§‹è¯­éŸ³è¯†åˆ«ï¼Œå†æ¬¡ç‚¹å‡»åœæ­¢')
      } catch (error) {
        console.error('å¯åŠ¨è¯­éŸ³è¯†åˆ«å¤±è´¥:', error)
        toast.error('å¯åŠ¨è¯­éŸ³è¯†åˆ«å¤±è´¥ï¼Œè¯·é‡è¯•')
        isPressingRef.current = false
        setIsListening(false)
      }
    }
  }

  // åœæ­¢è¯­éŸ³è¾“å…¥ï¼ˆç”¨äºè¿ç»­æ¨¡å¼è‡ªåŠ¨åœæ­¢ï¼‰
  const stopListening = () => {
    isPressingRef.current = false
    
    if (recognitionRef.current && isListening) {
      recognitionRef.current.stop()
    }
  }

  // åˆ‡æ¢è¿ç»­å¯¹è¯æ¨¡å¼
  const toggleContinuousMode = () => {
    const newMode = !continuousMode
    onContinuousModeChange?.(newMode)
    
    if (newMode) {
      toast.info('ğŸ“ å·²å¼€å¯è¿ç»­å¯¹è¯æ¨¡å¼ï¼ˆåƒæ‰“ç”µè¯ä¸€æ ·ï¼‰')
    } else {
      toast.info('å·²å…³é—­è¿ç»­å¯¹è¯æ¨¡å¼')
      stopListening()
    }
  }

  // æœ—è¯»æ–‡æœ¬ï¼ˆä½¿ç”¨è®¾ç½®ï¼‰
  const speak = (text: string) => {
    if (!synthesisRef.current || !text.trim()) return

    // åœæ­¢å½“å‰æœ—è¯»
    synthesisRef.current.cancel()

    const utterance = new SpeechSynthesisUtterance(text)
    utterance.lang = 'zh-CN'
    utterance.rate = voiceSettings.rate
    utterance.pitch = voiceSettings.pitch
    utterance.volume = voiceSettings.volume

    // è®¾ç½®éŸ³è‰²
    if (voiceSettings.voice !== 'default') {
      const selectedVoice = availableVoices.find((v) => v.name === voiceSettings.voice)
      if (selectedVoice) {
        utterance.voice = selectedVoice
      }
    }

    utterance.onstart = () => {
      setIsSpeaking(true)
    }

    utterance.onend = () => {
      setIsSpeaking(false)
      currentUtteranceRef.current = null
    }

    utterance.onerror = (event) => {
      console.error('è¯­éŸ³åˆæˆé”™è¯¯:', event)
      setIsSpeaking(false)
      toast.error('è¯­éŸ³æ’­æ”¾å¤±è´¥')
    }

    currentUtteranceRef.current = utterance
    synthesisRef.current.speak(utterance)
  }

  // åœæ­¢æœ—è¯»
  const stopSpeaking = () => {
    if (synthesisRef.current) {
      synthesisRef.current.cancel()
      setIsSpeaking(false)
      currentUtteranceRef.current = null
    }
  }

  // æš´éœ² speak æ–¹æ³•
  useEffect(() => {
    if (onSpeak) {
      // é€šè¿‡å›è°ƒæš´éœ²
      ;(onSpeak as any).speak = speak
    }
  }, [onSpeak, speak])

  if (!isSupported) {
    return null // ä¸æ”¯æŒæ—¶ä¸æ˜¾ç¤º
  }

  return (
    <div className="flex items-center gap-2">
      {/* è¿ç»­å¯¹è¯æ¨¡å¼åˆ‡æ¢ */}
      <Button
        type="button"
        variant={continuousMode ? 'default' : 'outline'}
        size="icon"
        onClick={toggleContinuousMode}
        title={continuousMode ? 'å…³é—­è¿ç»­å¯¹è¯' : 'å¼€å¯è¿ç»­å¯¹è¯ï¼ˆåƒæ‰“ç”µè¯ï¼‰'}
      >
        {continuousMode ? (
          <PhoneOff className="h-4 w-4" />
        ) : (
          <Phone className="h-4 w-4" />
        )}
      </Button>

      {/* è¯­éŸ³è¾“å…¥æŒ‰é’®ï¼ˆç‚¹å‡»åˆ‡æ¢æ¨¡å¼ï¼‰ */}
      <Button
        type="button"
        variant={isListening ? 'default' : 'outline'}
        size="icon"
        onClick={toggleListening}
        disabled={isSpeaking}
        className={isListening ? 'animate-pulse' : ''}
        title={isListening ? 'ç‚¹å‡»åœæ­¢è¯­éŸ³è¯†åˆ«' : 'ç‚¹å‡»å¼€å§‹è¯­éŸ³è¯†åˆ«'}
      >
        {isListening ? (
          <MicOff className="h-4 w-4" />
        ) : (
          <Mic className="h-4 w-4" />
        )}
      </Button>

      {/* è¯­éŸ³è¾“å‡ºæŒ‰é’® */}
      {isSpeaking ? (
        <Button
          type="button"
          variant="outline"
          size="icon"
          onClick={stopSpeaking}
          title="åœæ­¢æœ—è¯»"
        >
          <VolumeX className="h-4 w-4" />
        </Button>
      ) : (
        <Button
          type="button"
          variant="ghost"
          size="icon"
          disabled
          title="AI å›å¤å°†è‡ªåŠ¨æœ—è¯»"
        >
          <Volume2 className="h-4 w-4 opacity-50" />
        </Button>
      )}

      {/* è¯­éŸ³è®¾ç½® */}
      <VoiceSettingsDialog
        settings={voiceSettings}
        onSettingsChange={setVoiceSettings}
        availableVoices={availableVoices}
      />
    </div>
  )
}

// å¯¼å‡º speak å‡½æ•°ä¾›å¤–éƒ¨ä½¿ç”¨
export function useVoiceChat(settings?: VoiceSettings) {
  const currentSettings = settings || DEFAULT_SETTINGS
  const [availableVoices, setAvailableVoices] = useState<SpeechSynthesisVoice[]>([])

  useEffect(() => {
    if (!('speechSynthesis' in window)) return

    const loadVoices = () => {
      const voices = window.speechSynthesis.getVoices()
      setAvailableVoices(voices)
    }

    loadVoices()
    if ('onvoiceschanged' in window.speechSynthesis) {
      window.speechSynthesis.onvoiceschanged = loadVoices
    }
  }, [])

  const speak = (text: string) => {
    if (!('speechSynthesis' in window)) return

    const synthesis = window.speechSynthesis
    synthesis.cancel()

    const utterance = new SpeechSynthesisUtterance(text)
    utterance.lang = 'zh-CN'
    utterance.rate = currentSettings.rate
    utterance.pitch = currentSettings.pitch
    utterance.volume = currentSettings.volume

    if (currentSettings.voice !== 'default') {
      const selectedVoice = availableVoices.find((v) => v.name === currentSettings.voice)
      if (selectedVoice) {
        utterance.voice = selectedVoice
      }
    }

    synthesis.speak(utterance)
  }

  const stop = () => {
    if ('speechSynthesis' in window) {
      window.speechSynthesis.cancel()
    }
  }

  return { speak, stop }
}
